# ASTRAL_CORE 2.0 - Critical Alert Rules
# Life-saving monitoring with immediate escalation for failures

groups:
  # ========================================================================
  # CRISIS SYSTEM ALERTS - Highest Priority
  # ========================================================================
  - name: crisis_system_critical
    interval: 10s
    rules:
      - alert: CrisisEndpointDown
        expr: probe_success{job="crisis-endpoints"} == 0
        for: 30s
        labels:
          severity: critical
          team: crisis-response
          escalate: immediate
        annotations:
          summary: "CRITICAL: Crisis endpoint is down"
          description: "Crisis endpoint {{ $labels.instance }} has been down for {{ $value }} seconds. IMMEDIATE ACTION REQUIRED."
          runbook_url: "https://docs.astralcore.org/runbooks/crisis-endpoint-down"

      - alert: CrisisResponseTimeSlow
        expr: probe_duration_seconds{job="crisis-endpoints"} > 0.5
        for: 1m
        labels:
          severity: warning
          team: performance
        annotations:
          summary: "Crisis endpoint response time degraded"
          description: "Crisis endpoint {{ $labels.instance }} response time is {{ $value }}s (threshold: 0.5s)"

      - alert: CrisisWebSocketConnectionFailure
        expr: increase(websocket_connection_failures_total{service="crisis"}[5m]) > 10
        for: 2m
        labels:
          severity: critical
          team: crisis-response
        annotations:
          summary: "High number of WebSocket connection failures in crisis system"
          description: "{{ $value }} WebSocket connection failures in the last 5 minutes"

  # ========================================================================
  # APPLICATION HEALTH ALERTS
  # ========================================================================
  - name: application_health
    interval: 30s
    rules:
      - alert: ApplicationDown
        expr: up{job=~"astral-web|astral-admin"} == 0
        for: 1m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Application {{ $labels.job }} is down"
          description: "Application {{ $labels.job }} has been down for more than 1 minute"

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.instance }}"

      - alert: ResponseTimeHigh
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
          team: performance
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }}s for {{ $labels.instance }}"

  # ========================================================================
  # DATABASE ALERTS
  # ========================================================================
  - name: database_alerts
    interval: 30s
    rules:
      - alert: DatabaseDown
        expr: up{job="postgres"} == 0
        for: 30s
        labels:
          severity: critical
          team: database
          escalate: immediate
        annotations:
          summary: "CRITICAL: PostgreSQL database is down"
          description: "Database has been unreachable for {{ $value }} seconds"

      - alert: DatabaseConnectionsHigh
        expr: pg_stat_activity_count / pg_settings_max_connections > 0.8
        for: 5m
        labels:
          severity: warning
          team: database
        annotations:
          summary: "Database connection usage is high"
          description: "Database connections are at {{ $value | humanizePercentage }} of maximum"

      - alert: DatabaseSlowQueries
        expr: rate(pg_stat_activity_max_tx_duration[5m]) > 30
        for: 2m
        labels:
          severity: warning
          team: database
        annotations:
          summary: "Slow database queries detected"
          description: "Long-running queries detected (max duration: {{ $value }}s)"

  # ========================================================================
  # INFRASTRUCTURE ALERTS
  # ========================================================================
  - name: infrastructure_alerts
    interval: 60s
    rules:
      - alert: HighCPUUsage
        expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is {{ $value }}% on {{ $labels.instance }}"

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
        for: 5m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is {{ $value }}% on {{ $labels.instance }}"

      - alert: DiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Low disk space"
          description: "Disk usage is {{ $value }}% on {{ $labels.instance }}:{{ $labels.mountpoint }}"

  # ========================================================================
  # AI SAFETY ALERTS - Critical for crisis intervention platform
  # ========================================================================
  - name: ai_safety_alerts
    interval: 30s
    rules:
      - alert: AISafetyCheckFailure
        expr: increase(ai_safety_check_failures_total[5m]) > 3
        for: 1m
        labels:
          severity: critical
          team: ai-safety
          escalate: immediate
        annotations:
          summary: "CRITICAL: AI safety checks failing"
          description: "{{ $value }} AI safety check failures in 5 minutes - potential risk to users"
          action_required: "Review AI model outputs immediately, consider enabling fallback mode"

      - alert: HighRiskInterventionRequested
        expr: high_risk_intervention_requests_total > 10
        for: 2m
        labels:
          severity: warning
          team: crisis-response
        annotations:
          summary: "High number of high-risk intervention requests"
          description: "{{ $value }} high-risk interventions requested - elevated crisis activity"

  # ========================================================================
  # REDIS CACHE ALERTS
  # ========================================================================
  - name: redis_alerts
    interval: 30s
    rules:
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
          team: cache
        annotations:
          summary: "Redis is down"
          description: "Redis has been down for {{ $value }} seconds"

      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
          team: cache
        annotations:
          summary: "Redis memory usage is high"
          description: "Redis memory usage is {{ $value | humanizePercentage }}"

  # ========================================================================
  # SECURITY ALERTS
  # ========================================================================
  - name: security_alerts
    interval: 60s
    rules:
      - alert: AbnormalTrafficPattern
        expr: rate(nginx_http_requests_total[5m]) > 1000
        for: 2m
        labels:
          severity: warning
          team: security
        annotations:
          summary: "Abnormal traffic pattern detected"
          description: "Request rate is {{ $value }} requests/second"

      - alert: HighFailedLoginAttempts
        expr: rate(failed_login_attempts_total[5m]) > 10
        for: 2m
        labels:
          severity: warning
          team: security
        annotations:
          summary: "High number of failed login attempts"
          description: "{{ $value }} failed login attempts per second"

      - alert: SSLCertificateExpiringSoon
        expr: (ssl_certificate_expiry_seconds - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
          team: security
        annotations:
          summary: "SSL certificate expiring soon"
          description: "SSL certificate for {{ $labels.instance }} expires in {{ $value }} days"

  # ========================================================================
  # BUSINESS LOGIC ALERTS
  # ========================================================================
  - name: business_alerts
    interval: 300s
    rules:
      - alert: CrisisSessionsBacklog
        expr: crisis_sessions_pending > 50
        for: 5m
        labels:
          severity: warning
          team: crisis-response
        annotations:
          summary: "High number of pending crisis sessions"
          description: "{{ $value }} crisis sessions are pending volunteer assignment"

      - alert: NoVolunteersAvailable
        expr: volunteers_available_count < 2
        for: 2m
        labels:
          severity: critical
          team: crisis-response
          escalate: immediate
        annotations:
          summary: "CRITICAL: Very few volunteers available"
          description: "Only {{ $value }} volunteers available for crisis response"